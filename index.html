<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Avaloka Lens</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- TF.js + Object Detection -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<!-- MediaPipe OCR -->
<script type="module">
import { TextRecognizer, FilesetResolver }
from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest";

let video, canvas, ctx, explainBox;
let objectModel, textRecognizer;
let lastSpoken = "";

async function setupCamera() {
  video = document.getElementById("video");
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "environment" }
  });
  video.srcObject = stream;
  return new Promise(r => video.onloadedmetadata = r);
}

function explain(label, text) {
  if (label === "book") return "üìò This is a book. It contains knowledge for learning.";
  if (label === "bottle") return "üß¥ This is a bottle. It stores liquids.";
  if (label === "person") return "üë§ This is a human being.";

  if (text.includes("photosynthesis"))
    return "üå± Photosynthesis: plants make food using sunlight.";

  return label
    ? `üîç This looks like a ${label}.`
    : "üìñ I can read text from books and notes.";
}

function speak(text) {
  if (text === lastSpoken) return;
  lastSpoken = text;
  const u = new SpeechSynthesisUtterance(text);
  u.rate = 0.9;
  speechSynthesis.cancel();
  speechSynthesis.speak(u);
}

async function detectLoop() {
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

  const objects = await objectModel.detect(video);
  let label = "";

  objects.forEach(o => {
    label = o.class;
    ctx.strokeStyle = "#38bdf8";
    ctx.lineWidth = 2;
    ctx.strokeRect(...o.bbox);
    ctx.fillStyle = "#38bdf8";
    ctx.fillText(o.class, o.bbox[0], o.bbox[1] - 5);
  });

  const textResult = textRecognizer.recognize(video);
  const text = textResult.text?.toLowerCase() || "";

  const explanation = explain(label, text);
  explainBox.innerText = explanation;
  speak(explanation);

  requestAnimationFrame(detectLoop);
}

async function init() {
  await setupCamera();

  canvas = document.getElementById("canvas");
  ctx = canvas.getContext("2d");
  explainBox = document.getElementById("explain");

  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  objectModel = await cocoSsd.load();

  const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
  );

  textRecognizer = await TextRecognizer.createFromOptions(vision, {
    baseOptions: {
      modelAssetPath:
        "https://storage.googleapis.com/mediapipe-assets/text_recognizer.tflite"
    }
  });

  detectLoop();
}

window.onload = init;
</script>

<style>
body {
  margin: 0;
  background: #020617;
  color: #e5e7eb;
  font-family: system-ui;
  text-align: center;
}
h1 { color:#38bdf8; margin:10px; }
.container {
  position: relative;
  max-width: 420px;
  margin: auto;
  aspect-ratio: 3/4;
}
video, canvas {
  position:absolute;
  width:100%;
  height:100%;
}
.explain {
  position:fixed;
  bottom:12px;
  left:50%;
  transform:translateX(-50%);
  background:rgba(2,6,23,0.9);
  padding:14px 18px;
  border-radius:14px;
  max-width:92%;
}
</style>
</head>

<body>
<h1>üîç Avaloka Lens</h1>

<div class="container">
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas"></canvas>
</div>

<div class="explain" id="explain">
  Point the camera at an object or text üì∏
</div>
</body>
</html>
