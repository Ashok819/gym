<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Avaloka Vision ‚Äì ML + OCR</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  body {
    margin: 0;
    background: #020617;
    color: #e5e7eb;
    font-family: system-ui;
    text-align: center;
  }
  h1 {
    margin: 10px;
    color: #38bdf8;
  }
  .wrap {
    position: relative;
    max-width: 420px;
    margin: auto;
    aspect-ratio: 3 / 4;
  }
  video, canvas {
    position: absolute;
    width: 100%;
    height: 100%;
  }
  .info {
    position: fixed;
    bottom: 12px;
    left: 50%;
    transform: translateX(-50%);
    background: rgba(2,6,23,0.9);
    padding: 12px 16px;
    border-radius: 14px;
    max-width: 92%;
    font-size: 14px;
  }
</style>
</head>

<body>

<h1>üîç Avaloka Vision</h1>

<div class="wrap">
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas"></canvas>
</div>

<div class="info" id="info">Starting camera‚Ä¶</div>

<!-- TensorFlow -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<!-- OCR + ML Logic -->
<script type="module">
import { TextRecognizer, FilesetResolver }
from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest";

const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");
const info = document.getElementById("info");

let objectModel;
let textRecognizer;

async function startCamera() {
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "environment" }
  });
  video.srcObject = stream;
  return new Promise(r => video.onloadedmetadata = r);
}

function explain(label, text) {
  if (label === "book") return "üìò Book detected. It contains knowledge.";
  if (label === "person") return "üë§ Human detected.";
  if (label === "bottle") return "üß¥ Bottle detected.";

  if (text.includes("photosynthesis"))
    return "üå± Photosynthesis: plants make food using sunlight.";

  if (text.length > 10)
    return "üìñ Text detected. Looks educational.";

  return "üì∏ Point camera at an object or text";
}

async function detectLoop() {
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

  // Object detection
  const objects = await objectModel.detect(video);
  let label = "";

  objects.forEach(o => {
    label = o.class;
    ctx.strokeStyle = "#38bdf8";
    ctx.lineWidth = 2;
    ctx.strokeRect(...o.bbox);
    ctx.fillStyle = "#38bdf8";
    ctx.fillText(o.class, o.bbox[0], o.bbox[1] - 6);
  });

  // OCR
  const textResult = textRecognizer.recognize(video);
  const text = textResult.text?.toLowerCase() || "";

  info.innerText = explain(label, text);

  requestAnimationFrame(detectLoop);
}

async function init() {
  await startCamera();

  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  objectModel = await cocoSsd.load();

  const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
  );

  textRecognizer = await TextRecognizer.createFromOptions(vision, {
    baseOptions: {
      modelAssetPath:
        "https://storage.googleapis.com/mediapipe-assets/text_recognizer.tflite"
    }
  });

  info.innerText = "‚úÖ Camera + AI ready";
  detectLoop();
}

init().catch(err => {
  info.innerText = "‚ùå Error: " + err.message;
  alert(err.message);
});
</script>

</body>
</html>
